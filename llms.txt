# Red Hat OpenShift AI API Documentation

> Comprehensive guides for using Kubernetes APIs to create and manage OpenShift AI resources, including Data Science Projects and ML model deployment with KServe.

This documentation provides practical, YAML-based tutorials for deploying AI/ML workloads on OpenShift using native Kubernetes APIs. It focuses on real-world scenarios with GPU-accelerated model serving using vLLM and Red Hat's pre-configured model containers.

## Docs

- [Setting Up PVCs for Model Storage](https://cfchase.github.io/rhoai-api-docs/docs/extras/model-pvc-setup): This guide explains how to create and configure Persistent Volume Claims (PVCs) for storing large language models that can be served by OpenShift AI. Using PVCs allows you to store models on shared storage and serve them across multiple deployments.
- [Serving Large Language Models (LLMs)](https://cfchase.github.io/rhoai-api-docs/docs/serving/llms): Red Hat OpenShift AI provides a comprehensive solution for deploying and serving Large Language Models (LLMs) at scale. Using the KServe infrastructure, you can deploy models from various storage sources including pre-built ModelCars (OCI registries), S3-compatible storage, and Persistent Volume Claims (PVCs).
- [Projects](https://cfchase.github.io/rhoai-api-docs/docs/projects): Data Science Projects in Red Hat OpenShift AI provide isolated environments for organizing your machine learning work. These projects are OpenShift projects (Kubernetes namespaces) with specific labels and annotations that enable integration with the OpenShift AI dashboard and features.
- [Data Connections](https://cfchase.github.io/rhoai-api-docs/docs/data-connections): Data Connections in Red Hat OpenShift AI provide secure access to external data sources and model registries. These connections enable workbenches, model serving, and pipelines to access S3-compatible object storage, model files on persistent volumes, and container registries without embedding credentials directly in your code.
- [Serving](https://cfchase.github.io/rhoai-api-docs/docs/serving): This section covers model serving capabilities in Red Hat OpenShift AI, including deployment strategies, configuration options, and best practices for serving machine learning models at scale.
- [Useful Extras](https://cfchase.github.io/rhoai-api-docs/docs/extras): This section contains supplementary guides and how-to documents that complement the main documentation. These guides provide detailed instructions for specific tasks that support multiple use cases across OpenShift AI.

