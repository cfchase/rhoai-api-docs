# Red Hat OpenShift AI API Documentation

> Comprehensive guides for using Kubernetes APIs to create and manage OpenShift AI resources, including Data Science Projects and ML model deployment with KServe.

This documentation provides practical, YAML-based tutorials for deploying AI/ML workloads on OpenShift using native Kubernetes APIs. It focuses on real-world scenarios with GPU-accelerated model serving using vLLM and Red Hat's pre-configured model containers.

## Docs

- [ModelCars](https://cfchase.github.io/rhoai-api-docs/docs/serving/modelcar): ModelCar serving in Red Hat OpenShift AI provides a comprehensive solution for deploying and serving machine learning models at scale. Using the KServe infrastructure, ModelCars enable production-ready model deployments with features like GPU acceleration, autoscaling, and secure external access.
- [Projects](https://cfchase.github.io/rhoai-api-docs/docs/projects): Data Science Projects in Red Hat OpenShift AI provide isolated environments for organizing your machine learning work. These projects are OpenShift projects (Kubernetes namespaces) with specific labels and annotations that enable integration with the OpenShift AI dashboard and features.
- [Data Connections](https://cfchase.github.io/rhoai-api-docs/docs/data-connections): Data Connections in Red Hat OpenShift AI provide secure access to external data sources and model registries. These connections enable workbenches, model serving, and pipelines to access S3-compatible object storage, model files on persistent volumes, and container registries without embedding credentials directly in your code.
- [Serving](https://cfchase.github.io/rhoai-api-docs/docs/serving): This section covers model serving capabilities in Red Hat OpenShift AI, including deployment strategies, configuration options, and best practices for serving machine learning models at scale.

