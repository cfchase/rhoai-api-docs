# Setting Up PVCs for Model Storage

This guide explains how to create and configure Persistent Volume Claims (PVCs) for storing large language models that can be served by OpenShift AI. Using PVCs allows you to store models on shared storage and serve them across multiple deployments.

## Overview

PVC-based model storage is useful when you:
- Have models stored on shared file systems
- Need to modify or customize model files
- Want to avoid downloading models from external sources repeatedly
- Require ReadWriteMany (RWX) access for multiple pods

## Prerequisites

- Access to a Kubernetes/OpenShift cluster
- Storage class that supports RWX access mode (e.g., NFS, CephFS, GlusterFS)
- Sufficient storage quota for your models (LLMs can be 10-100+ GB)
- `kubectl` or `oc` CLI tool configured

## Creating a PVC for Model Storage

### Step 1: Determine Storage Requirements

First, check available storage classes that support RWX:

```bash
# List storage classes
kubectl get storageclass

# Check which support RWX (look for "ReadWriteMany" in the output)
kubectl describe storageclass <storage-class-name>
```

### Step 2: Create the PVC

Create a PVC with sufficient capacity for your models:

```yaml
# model-storage-pvc.yaml
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: model-pvc
  namespace: my-namespace
  labels:
    app: model-storage
    purpose: llm-serving
spec:
  accessModes:
    - ReadWriteMany  # Required for multiple pod access
  resources:
    requests:
      storage: 100Gi  # Adjust based on model size
  storageClassName: nfs-storage  # Replace with your RWX storage class
```

Apply the PVC:
```bash
kubectl apply -f model-storage-pvc.yaml -n my-namespace
```

Verify PVC is bound:
```bash
kubectl get pvc model-pvc -n my-namespace

# Expected output:
NAME        STATUS   VOLUME                                     CAPACITY   ACCESS MODES   STORAGECLASS   AGE
model-pvc   Bound    pvc-12345678-abcd-efgh-ijkl-123456789012   100Gi      RWX            nfs-storage    1m
```

## Downloading Models to PVC

### Method 1: Using a Job to Download Models

Create a Kubernetes Job to download models directly to the PVC:

```yaml
# download-model-job.yaml
apiVersion: batch/v1
kind: Job
metadata:
  name: download-llama-model
  namespace: my-namespace
spec:
  template:
    spec:
      containers:
      - name: downloader
        image: python:3.11-slim
        command: ["/bin/bash", "-c"]
        args:
          - |
            # Install required tools
            pip install huggingface-hub
            
            # Download model to PVC
            python -c "
            from huggingface_hub import snapshot_download
            
            # Download Llama-2-7B model
            snapshot_download(
                repo_id='meta-llama/Llama-2-7b-chat-hf',
                local_dir='/models/llama-2-7b-chat',
                local_dir_use_symlinks=False,
                token='YOUR_HF_TOKEN'  # Replace with your Hugging Face token
            )
            "
            
            echo "Model download complete!"
            ls -la /models/llama-2-7b-chat/
        volumeMounts:
        - name: model-storage
          mountPath: /models
        resources:
          requests:
            memory: "4Gi"
            cpu: "2"
          limits:
            memory: "8Gi"
            cpu: "4"
      volumes:
      - name: model-storage
        persistentVolumeClaim:
          claimName: model-pvc
      restartPolicy: Never
  backoffLimit: 2
```

Run the job:
```bash
kubectl apply -f download-model-job.yaml -n my-namespace

# Monitor progress
kubectl logs -f job/download-llama-model -n my-namespace
```

### Method 2: Using a Temporary Pod

Create a temporary pod to manually download or copy models:

```yaml
# model-setup-pod.yaml
apiVersion: v1
kind: Pod
metadata:
  name: model-setup
  namespace: my-namespace
spec:
  containers:
  - name: setup
    image: python:3.11
    command: ["sleep", "infinity"]
    volumeMounts:
    - name: model-storage
      mountPath: /models
    resources:
      requests:
        memory: "4Gi"
        cpu: "2"
  volumes:
  - name: model-storage
    persistentVolumeClaim:
      claimName: model-pvc
```

Deploy and access the pod:
```bash
# Create the pod
kubectl apply -f model-setup-pod.yaml -n my-namespace

# Wait for pod to be ready
kubectl wait --for=condition=ready pod/model-setup -n my-namespace

# Access the pod
kubectl exec -it model-setup -n my-namespace -- bash

# Inside the pod, download models
pip install huggingface-hub
python -c "from huggingface_hub import snapshot_download; snapshot_download('meta-llama/Llama-2-7b-chat-hf', local_dir='/models/llama-2-7b-chat', local_dir_use_symlinks=False)"

# Exit and delete the pod when done
exit
kubectl delete pod model-setup -n my-namespace
```

### Method 3: Copying from Local Machine

If you have models locally, copy them to the PVC:

```bash
# Create a temporary pod with the PVC mounted
kubectl run model-copy --image=busybox --restart=Never --rm -i --tty \
  --overrides='{"spec":{"volumes":[{"name":"model-storage","persistentVolumeClaim":{"claimName":"model-pvc"}}],"containers":[{"name":"model-copy","volumeMounts":[{"name":"model-storage","mountPath":"/models"}]}]}}' \
  -n my-namespace -- sh

# In another terminal, copy files to the pod
kubectl cp /local/path/to/model my-namespace/model-copy:/models/my-model

# The pod will automatically be deleted when you exit
```

## Verifying Model Files

Check that models are correctly stored on the PVC:

```bash
# Create a debug pod to inspect the PVC
kubectl run pvc-inspector --image=busybox --restart=Never --rm -i --tty \
  --overrides='{"spec":{"volumes":[{"name":"model-storage","persistentVolumeClaim":{"claimName":"model-pvc"}}],"containers":[{"name":"pvc-inspector","volumeMounts":[{"name":"model-storage","mountPath":"/models"}]}]}}' \
  -n my-namespace -- sh

# Inside the pod
ls -la /models/
du -sh /models/*
```

## Using PVC Storage in InferenceService

Once models are stored on the PVC, reference them in your InferenceService:

```yaml
apiVersion: serving.kserve.io/v1beta1
kind: InferenceService
metadata:
  name: llama-model
  namespace: my-namespace
spec:
  predictor:
    model:
      modelFormat:
        name: vLLM
      runtime: llama-runtime
      # Reference the PVC and model path
      storageUri: 'pvc://model-pvc/llama-2-7b-chat'
      resources:
        requests:
          nvidia.com/gpu: '1'
        limits:
          nvidia.com/gpu: '1'
```

## Best Practices

### Storage Sizing

- **Small models (< 10GB)**: 50Gi PVC
- **Medium models (10-50GB)**: 100Gi PVC
- **Large models (> 50GB)**: 200Gi+ PVC
- Add 20% overhead for temporary files and caching

### Access Modes

- Use **ReadWriteMany (RWX)** for:
  - Serving models from multiple pods
  - Updating models without downtime
  - Shared model repositories

- Use **ReadWriteOnce (RWO)** only if:
  - Single pod deployment
  - Cost is a major concern
  - RWX is not available

### Organization

Structure your models on the PVC:
```
/models/
├── llama-2-7b-chat/
│   ├── config.json
│   ├── model.safetensors
│   └── tokenizer.json
├── granite-3-1-8b/
│   └── ...
└── mistral-7b/
    └── ...
```

### Performance Considerations

1. **Storage Class**: Choose high-performance storage for production
2. **Caching**: Models are loaded into memory, so initial load time is most important
3. **Network**: Ensure good network connectivity between nodes and storage

## Troubleshooting

### PVC Won't Bind

```bash
# Check PVC events
kubectl describe pvc model-pvc -n my-namespace

# Common issues:
# - No storage class supports RWX
# - Insufficient quota
# - Storage class doesn't exist
```

### Model Loading Errors

```bash
# Check InferenceService logs
kubectl logs -l serving.kserve.io/inferenceservice=your-model -n my-namespace

# Common issues:
# - Wrong path in storageUri
# - Missing model files
# - Incorrect permissions
```

### Slow Model Loading

- Check storage performance: `kubectl exec -it <pod> -- dd if=/models/test of=/dev/null bs=1M count=1000`
- Consider using higher performance storage class
- Ensure nodes have good network connectivity to storage

## Related Resources

- [Serving Large Language Models](/docs/serving/llms/)
- [Data Connections](/docs/data-connections/)
- [Kubernetes Persistent Volumes Documentation](https://kubernetes.io/docs/concepts/storage/persistent-volumes/)

# Serving Large Language Models (LLMs)

Red Hat OpenShift AI provides a comprehensive solution for deploying and serving Large Language Models (LLMs) at scale. Using the KServe infrastructure, you can deploy models from various storage sources including pre-built ModelCars (OCI registries), S3-compatible storage, and Persistent Volume Claims (PVCs).

## Overview

A complete model serving deployment in OpenShift AI consists of three key resources working together:

1. **ServingRuntime**: Defines HOW models are served
   - Specifies the serving container (e.g., vLLM, Triton, MLServer)
   - Configures runtime parameters and environment
   - Sets resource requirements and hardware acceleration

2. **InferenceService**: Defines WHAT model to serve
   - References the ServingRuntime to use
   - Specifies the model location (storageUri or storage)
   - Controls deployment parameters (replicas, resources, tolerations)
   - Manages the model lifecycle

3. **Route** (Optional): Provides external access
   - Exposes the model endpoint outside the cluster
   - Configures TLS termination
   - Enables secure external access

These resources must be created together to deploy a functional model serving endpoint. **Important**: The ServingRuntime and InferenceService names MUST match exactly - this is a requirement for the OpenShift AI dashboard to properly display and manage the model deployment. Together they create the pods, services, and other Kubernetes resources needed to serve your model.

## Important: Naming Convention

**The ServingRuntime and InferenceService names MUST be identical.** This is a critical requirement for the OpenShift AI dashboard to properly:
- Display the model deployment
- Show serving runtime details
- Enable management features
- Link the resources correctly

For example, if your InferenceService is named `granite-model`, then your ServingRuntime must also be named `granite-model`. This naming convention is enforced throughout all examples in this guide.

### Template Variables in ServingRuntime

OpenShift AI provides template variables that get replaced at runtime with values from the InferenceService:

- `{{.Name}}` - The name of the InferenceService
- Common usage: `--served-model-name={{.Name}}` in vLLM args

This ensures the model server uses the same name as the InferenceService, maintaining consistency across the deployment.

## Prerequisites

Before deploying models with ModelCars, ensure you have:

1. **OpenShift AI cluster** with appropriate permissions
2. **GPU nodes** (if using GPU-accelerated models)
3. **CLI tools**: `kubectl` or `oc` installed and configured
4. **Namespace access**: Create or use an existing namespace for deployments
5. **NVIDIA GPU operator** installed (for GPU workloads)

### Checking Available AcceleratorProfiles

AcceleratorProfiles define GPU configurations available in your cluster. While optional, understanding available profiles helps configure proper GPU resource requests and node tolerations.

#### List AcceleratorProfiles

```bash
# List all accelerator profiles
kubectl get acceleratorprofiles -n redhat-ods-applications

# Example output:
NAME           DISPLAY NAME    ENABLED   IDENTIFIER        TOLERATIONS
migrated-gpu   NVIDIA GPU      true      nvidia.com/gpu    1 toleration(s)
nvidia-a100    NVIDIA A100     true      nvidia.com/gpu    1 toleration(s)
```

#### View Specific AcceleratorProfile

```bash
# Get detailed profile information
kubectl get acceleratorprofile migrated-gpu -n redhat-ods-applications -o yaml

# Or use describe for a summary
kubectl describe acceleratorprofile migrated-gpu -n redhat-ods-applications
```

Example AcceleratorProfile:
```yaml
apiVersion: dashboard.opendatahub.io/v1
kind: AcceleratorProfile
metadata:
  name: migrated-gpu
  namespace: redhat-ods-applications
spec:
  displayName: "NVIDIA GPU"
  enabled: true
  identifier: nvidia.com/gpu      # GPU resource identifier
  tolerations:                     # Node scheduling tolerations
  - effect: NoSchedule
    key: nvidia.com/gpu
    operator: Exists
```

#### Extract Configuration for Model Deployment

From the AcceleratorProfile, note:
- **identifier**: Use in `resources.requests` and `resources.limits` (e.g., `nvidia.com/gpu: '1'`)
- **tolerations**: Copy to your InferenceService spec for proper node scheduling

If no AcceleratorProfiles exist, you can still deploy models by manually specifying GPU resources and tolerations based on your cluster configuration.

## Storage Options for LLMs

OpenShift AI supports three primary storage methods for serving LLMs. Each method has specific use cases and configuration requirements:

### 1. ModelCars (OCI Registry)

Pre-built, optimized container images from Red Hat's registry or other OCI-compliant registries.

**When to use:**
- Quick deployment of Red Hat validated models
- Models that are pre-optimized and containerized
- When you don't need to modify model files

**Configuration:**
```yaml
spec:
  predictor:
    model:
      storageUri: 'oci://registry.redhat.io/rhelai1/modelcar-granite-3-1-8b-instruct:1.5'
```

### 2. S3-Compatible Storage

Models stored in S3-compatible object storage (AWS S3, MinIO, OpenShift Data Foundation).

**When to use:**
- Models stored in object storage
- When you need to share models across multiple deployments
- Integration with existing S3 infrastructure

**Configuration:**
```yaml
spec:
  predictor:
    model:
      storage:
        key: aws-connection-my-storage  # Name of your S3 data connection
        path: models/granite-7b-instruct/  # Path within the S3 bucket
```

**Prerequisites:**
- Create an S3 data connection in your namespace (see [Data Connections](/docs/data-connections/))
- Ensure the model files are uploaded to the specified S3 path

### 3. Persistent Volume Claims (PVC)

Models stored on Kubernetes persistent volumes.

**When to use:**
- Models already available on shared storage
- When you need ReadWriteMany (RWX) access
- Custom model files or modified versions

**Configuration:**
```yaml
spec:
  predictor:
    model:
      storageUri: 'pvc://model-pvc/llama-2-7b-chat'  # pvc://<pvc-name>/<model-path>
```

**Prerequisites:**
- PVC must exist in the same namespace
- Model files must be copied to the PVC (see [Setting Up PVCs for Model Storage](/docs/extras/model-pvc-setup/))

### Choosing a Storage Method

| Storage Type | Best For | Pros | Cons |
|-------------|----------|------|------|
| **ModelCars (OCI)** | Production deployments | Pre-optimized, versioned, easy to deploy | Limited to available images |
| **S3 Storage** | Shared models, cloud environments | Centralized storage, easy updates | Requires S3 setup, potential latency |
| **PVC Storage** | Custom models, on-premise | Full control, no external dependencies | Requires PVC management, manual uploads |

## Creating Model Deployments

Model deployments require creating both a ServingRuntime and InferenceService. Optionally, you can also create a Route for external access. The resources should be created in order: ServingRuntime → InferenceService → Route.

### Method 1: Declarative (Using YAML)

The declarative approach uses YAML files to define your model serving stack. This method is recommended for:
- Version control and GitOps workflows
- Reproducible deployments
- Production environments
- Automated provisioning

#### Basic Model Deployment

The following examples show how to deploy the same model using different storage options. Choose the one that matches your storage method:

```yaml
# serving-basic.yaml
apiVersion: v1
kind: List
items:
  # ServingRuntime with GPU support
  - apiVersion: serving.kserve.io/v1alpha1
    kind: ServingRuntime
    metadata:
      name: granite-model  # Must match InferenceService name
      annotations:
        # GPU-specific annotations for dashboard integration
        opendatahub.io/accelerator-name: migrated-gpu  # Links to AcceleratorProfile
        opendatahub.io/recommended-accelerators: '["nvidia.com/gpu"]'  # GPU types supported
        openshift.io/display-name: Granite Model Runtime
      labels:
        opendatahub.io/dashboard: 'true'  # Shows in OpenShift AI dashboard
    spec:
      annotations:
        # Prometheus monitoring for production deployments
        prometheus.io/path: /metrics
        prometheus.io/port: '8080'
      containers:
        - name: kserve-container
          image: 'quay.io/modh/vllm:rhoai-2.20-cuda'  # CUDA-enabled vLLM image
          args:
            - '--port=8080'
            - '--model=/mnt/models'
            - '--served-model-name={{.Name}}'  # Uses InferenceService name
            - '--tensor-parallel-size=1'  # GPU parallelism setting
          command:
            - python
            - '-m'
            - vllm.entrypoints.openai.api_server
          env:
            - name: HF_HOME
              value: /tmp/hf_home  # Cache directory for model files
          ports:
            - containerPort: 8080
              protocol: TCP
          volumeMounts:
            - mountPath: /dev/shm  # Shared memory for GPU operations
              name: shm
      supportedModelFormats:
        - name: vLLM
          autoSelect: true
      volumes:
        - name: shm  # Shared memory volume for GPU performance
          emptyDir:
            medium: Memory
            sizeLimit: 2Gi

  # InferenceService with GPU resources
  - apiVersion: serving.kserve.io/v1beta1
    kind: InferenceService
    metadata:
      name: granite-model
      annotations:
        openshift.io/display-name: Granite 3.1 8B Model
        serving.kserve.io/deploymentMode: RawDeployment  # Direct pod deployment
      labels:
        opendatahub.io/dashboard: 'true'
    spec:
      predictor:
        minReplicas: 1
        maxReplicas: 1
        model:
          modelFormat:
            name: vLLM
          runtime: granite-model  # Must match InferenceService name
          # Option 1: ModelCar (OCI Registry)
          storageUri: 'oci://registry.redhat.io/rhelai1/modelcar-granite-3-1-8b-instruct:1.5'
          # Option 2: S3 Storage (comment out storageUri above and use this instead)
          # storage:
          #   key: aws-connection-my-storage  # Your S3 data connection name
          #   path: models/granite-3-1-8b-instruct/  # Path in S3 bucket
          # Option 3: PVC Storage (comment out storageUri above and use this instead)  
          # storageUri: 'pvc://model-pvc/granite-3-1-8b-instruct'  # PVC name and model path
          args:
            - '--max-model-len=4096'  # Model-specific context length
          resources:  # GPU resource requirements
            requests:
              cpu: '2'
              memory: 16Gi
              nvidia.com/gpu: '1'  # Request 1 NVIDIA GPU
            limits:
              cpu: '8'
              memory: 24Gi
              nvidia.com/gpu: '1'  # Limit to 1 NVIDIA GPU
        tolerations:  # GPU node scheduling
          - effect: NoSchedule
            key: nvidia.com/gpu
            operator: Exists
```

Apply the deployment:
```bash
kubectl apply -f serving-basic.yaml -n my-namespace
```

#### Alternative Runtime Images

The examples above use `quay.io/modh/vllm:rhoai-2.20-cuda` for NVIDIA GPUs. OpenShift AI provides different runtime images for various architectures:

- **NVIDIA GPUs**: `quay.io/modh/vllm:rhoai-2.20-cuda`
- **AMD GPUs**: `quay.io/modh/vllm:rhoai-2.20-rocm`
- **Intel Habana**: `quay.io/modh/vllm:rhoai-2.20-gaudi`
- **CPU only**: `quay.io/modh/vllm:rhoai-2.20-cpu`

Example deployment for AMD GPUs:

```yaml
# serving-amd-gpu.yaml
apiVersion: v1
kind: List
items:
  - apiVersion: serving.kserve.io/v1alpha1
    kind: ServingRuntime
    metadata:
      name: amd-model  # Must match InferenceService name
      annotations:
        # AMD GPU-specific accelerator type
        opendatahub.io/recommended-accelerators: '["amd.com/gpu"]'  # AMD instead of NVIDIA
      labels:
        opendatahub.io/dashboard: 'true'
    spec:
      containers:
        - name: kserve-container
          # ROCm-specific image for AMD GPU support
          image: 'quay.io/modh/vllm:rhoai-2.20-rocm'  # ROCm instead of CUDA
          args:
            - '--port=8080'
            - '--model=/mnt/models'
            - '--served-model-name={{.Name}}'
          command:
            - python
            - '-m'
            - vllm.entrypoints.openai.api_server
          resources:
            limits:
              amd.com/gpu: '1'  # AMD GPU resource identifier
      supportedModelFormats:
        - name: vLLM
          autoSelect: true
  
  - apiVersion: serving.kserve.io/v1beta1
    kind: InferenceService
    metadata:
      name: amd-model
      labels:
        opendatahub.io/dashboard: 'true'
    spec:
      predictor:
        model:
          modelFormat:
            name: vLLM
          runtime: amd-model
          storageUri: 'oci://registry.redhat.io/rhelai1/modelcar-granite-3-1-8b-instruct:1.5'
          resources:
            limits:
              amd.com/gpu: '1'  # Must match the GPU type in ServingRuntime
```

#### Example: S3 Storage Deployment

Deploy a model from S3-compatible storage using a data connection:

```yaml
# serving-s3-storage.yaml
apiVersion: v1
kind: List
items:
  # ServingRuntime (same as basic example)
  - apiVersion: serving.kserve.io/v1alpha1
    kind: ServingRuntime
    metadata:
      name: s3-model
      labels:
        opendatahub.io/dashboard: 'true'
    spec:
      containers:
        - name: kserve-container
          image: 'quay.io/modh/vllm:rhoai-2.20-cuda'
          args:
            - '--port=8080'
            - '--model=/mnt/models'
            - '--served-model-name={{.Name}}'
            - '--tensor-parallel-size=1'
          resources:
            limits:
              nvidia.com/gpu: '1'
      supportedModelFormats:
        - name: vLLM
          autoSelect: true
  
  # InferenceService using S3 storage
  - apiVersion: serving.kserve.io/v1beta1
    kind: InferenceService
    metadata:
      name: s3-model
      labels:
        opendatahub.io/dashboard: 'true'
    spec:
      predictor:
        model:
          modelFormat:
            name: vLLM
          runtime: s3-model
          # Use storage field for S3 connections
          storage:
            key: aws-connection-my-storage  # Name of your S3 data connection
            path: models/granite-7b-instruct/  # Path within the S3 bucket
          args:
            - '--max-model-len=4096'
          resources:
            requests:
              nvidia.com/gpu: '1'
            limits:
              nvidia.com/gpu: '1'
        tolerations:
          - effect: NoSchedule
            key: nvidia.com/gpu
            operator: Exists
```

**Prerequisites for S3 storage:**
1. Create an S3 data connection named `aws-connection-my-storage` (see [Data Connections](/docs/data-connections/))
2. Upload your model files to `s3://your-bucket/models/granite-7b-instruct/`

#### Example: PVC Storage Deployment

Deploy a model from a Persistent Volume Claim:

```yaml
# serving-pvc-storage.yaml
apiVersion: v1
kind: List
items:
  # ServingRuntime (same as basic example)
  - apiVersion: serving.kserve.io/v1alpha1
    kind: ServingRuntime
    metadata:
      name: pvc-model
      labels:
        opendatahub.io/dashboard: 'true'
    spec:
      containers:
        - name: kserve-container
          image: 'quay.io/modh/vllm:rhoai-2.20-cuda'
          args:
            - '--port=8080'
            - '--model=/mnt/models'
            - '--served-model-name={{.Name}}'
            - '--tensor-parallel-size=1'
          resources:
            limits:
              nvidia.com/gpu: '1'
      supportedModelFormats:
        - name: vLLM
          autoSelect: true
  
  # InferenceService using PVC storage
  - apiVersion: serving.kserve.io/v1beta1
    kind: InferenceService
    metadata:
      name: pvc-model
      labels:
        opendatahub.io/dashboard: 'true'
    spec:
      predictor:
        model:
          modelFormat:
            name: vLLM
          runtime: pvc-model
          # Use storageUri with pvc:// scheme
          storageUri: 'pvc://model-pvc/llama-2-7b-chat'  # pvc://<pvc-name>/<model-path>
          args:
            - '--max-model-len=4096'
          resources:
            requests:
              nvidia.com/gpu: '1'
            limits:
              nvidia.com/gpu: '1'
        tolerations:
          - effect: NoSchedule
            key: nvidia.com/gpu
            operator: Exists
```

**Prerequisites for PVC storage:**
1. Create a PVC named `model-pvc` with RWX access mode
2. Copy model files to the PVC at path `llama-2-7b-chat/`
3. See [Setting Up PVCs for Model Storage](/docs/extras/model-pvc-setup/) for detailed instructions

#### Advanced Model Deployment with Route and Authentication

A complete production deployment with external access and authentication:

```yaml
# serving-advanced.yaml
apiVersion: v1
kind: List
items:
  # ServingRuntime with full production configuration
  - apiVersion: serving.kserve.io/v1alpha1
    kind: ServingRuntime
    metadata:
      name: llama-model  # Must match InferenceService name
      annotations:
        # Advanced GPU configuration
        opendatahub.io/accelerator-name: nvidia-a100  # Specific GPU model
        opendatahub.io/apiProtocol: REST
        opendatahub.io/recommended-accelerators: '["nvidia.com/gpu"]'
        openshift.io/display-name: Llama Model Runtime
      labels:
        opendatahub.io/dashboard: 'true'
        environment: production  # Custom label for environment tracking
    spec:
      annotations:
        # Enhanced monitoring configuration
        prometheus.io/path: /metrics
        prometheus.io/port: '8080'
        prometheus.io/scrape: 'true'  # Enable metrics collection
      containers:
        - name: kserve-container
          image: 'quay.io/modh/vllm:rhoai-2.20-cuda'
          args:
            - '--port=8080'
            - '--model=/mnt/models'
            - '--served-model-name={{.Name}}'
            # Advanced vLLM configuration for large models
            - '--tensor-parallel-size=2'  # Split model across 2 GPUs
            - '--max-model-len=8192'      # Extended context window
            - '--max-num-seqs=256'        # High concurrent request support
          command:
            - python
            - '-m'
            - vllm.entrypoints.openai.api_server
          env:
            - name: HF_HOME
              value: /tmp/hf_home
            # Performance optimization
            - name: VLLM_ATTENTION_BACKEND
              value: FLASHINFER  # Optimized attention mechanism
          ports:
            - containerPort: 8080
              name: http  # Named port for readiness probe
              protocol: TCP
          volumeMounts:
            - mountPath: /dev/shm
              name: shm
          # Production readiness check
          readinessProbe:
            httpGet:
              path: /health
              port: http
            initialDelaySeconds: 30
            periodSeconds: 10
      supportedModelFormats:
        - name: vLLM
          autoSelect: true
      volumes:
        - name: shm
          emptyDir:
            medium: Memory
            sizeLimit: 12Gi  # Larger shared memory for 70B model

  # InferenceService with authentication and autoscaling
  - apiVersion: serving.kserve.io/v1beta1
    kind: InferenceService
    metadata:
      name: llama-model
      annotations:
        openshift.io/display-name: Llama 3.3 70B Model
        # Security and authentication
        security.opendatahub.io/enable-auth: 'true'  # Enable OpenShift AI auth
        serving.kserve.io/deploymentMode: RawDeployment
        serving.kserve.io/enable-prometheus-scraping: 'true'
      labels:
        # Required for route exposure
        networking.kserve.io/visibility: exposed  # Allow external access
        opendatahub.io/dashboard: 'true'
        environment: production
    spec:
      predictor:
        # Autoscaling configuration
        minReplicas: 1
        maxReplicas: 3     # Scale up to 3 instances
        scaleTarget: 80    # Target 80% CPU utilization
        scaleMetric: cpu   # Scale based on CPU usage
        model:
          modelFormat:
            name: vLLM
          runtime: llama-model  # Must match InferenceService name
          storageUri: 'oci://registry.redhat.io/rhelai1/modelcar-llama-3-3-70b-instruct:1.5'
          args:
            # Model-specific optimization
            - '--max-model-len=8192'
            - '--gpu-memory-utilization=0.95'  # Use 95% of GPU memory
          resources:
            requests:
              cpu: '8'
              memory: 80Gi
              nvidia.com/gpu: '2'
            limits:
              cpu: '16'
              memory: 96Gi
              nvidia.com/gpu: '2'
        tolerations:
          - effect: NoSchedule
            key: nvidia.com/gpu
            operator: Exists
          - effect: NoSchedule
            key: nvidia.com/gpu-model
            operator: Equal
            value: A100

  # Route for external access
  - apiVersion: route.openshift.io/v1
    kind: Route
    metadata:
      name: llama-model
      labels:
        inferenceservice-name: llama-model
    spec:
      to:
        kind: Service
        name: llama-model-predictor
        weight: 100
      port:
        targetPort: http
      tls:
        termination: edge
        insecureEdgeTerminationPolicy: Redirect
      wildcardPolicy: None
```

### Method 2: Imperative (Using Commands)

While declarative deployment is preferred, you can create resources imperatively for quick testing or development.

#### Create ServingRuntime

```bash
# Create a basic ServingRuntime
kubectl create -f - <<EOF
apiVersion: serving.kserve.io/v1alpha1
kind: ServingRuntime
metadata:
  name: test-model  # Must match InferenceService name
spec:
  containers:
    - name: kserve-container
      image: 'quay.io/modh/vllm:rhoai-2.20-cuda'
      args: ['--port=8080', '--model=/mnt/models']
  supportedModelFormats:
    - name: vLLM
      autoSelect: true
EOF
```

#### Create InferenceService

```bash
# Create InferenceService after ServingRuntime exists
kubectl create -f - <<EOF
apiVersion: serving.kserve.io/v1beta1
kind: InferenceService
metadata:
  name: test-model
spec:
  predictor:
    model:
      modelFormat:
        name: vLLM
      runtime: test-model  # Must match InferenceService name
      storageUri: 'oci://registry.redhat.io/rhelai1/modelcar-granite-3-1-8b-instruct:1.5'
EOF
```

#### Create Route (Optional)

```bash
# Expose the model externally
kubectl expose service test-model-predictor \
  --name=test-model-route \
  --port=80 \
  --target-port=8080 \
  --type=LoadBalancer

# Or create an OpenShift Route
oc expose service test-model-predictor
```

## Listing and Viewing Model Deployments

### List All Model Deployments

List all InferenceServices (the primary resource representing model deployments):

```bash
# List all InferenceServices in current namespace
kubectl get inferenceservices

# List with more details
kubectl get inferenceservices -o wide

# List across all namespaces
kubectl get inferenceservices --all-namespaces

# Custom output showing key fields
kubectl get inferenceservices -o custom-columns=\
NAME:.metadata.name,\
RUNTIME:.spec.predictor.model.runtime,\
MODEL:.spec.predictor.model.storageUri,\
READY:.status.conditions[?(@.type=='Ready')].status,\
URL:.status.url
```

### List Associated Resources

```bash
# List all ServingRuntimes
kubectl get servingruntimes

# List Routes for external access
kubectl get routes -l inferenceservice-name

# List all resources for a specific model deployment
MODEL_NAME="granite-model"
kubectl get servingruntime,inferenceservice,route,service,deployment,pod \
  -l serving.kserve.io/inferenceservice=$MODEL_NAME
```

### View Specific Model Deployment

```bash
# View InferenceService details
kubectl describe inferenceservice granite-model

# Get InferenceService in YAML format
kubectl get inferenceservice granite-model -o yaml

# View ServingRuntime details
kubectl describe servingruntime granite-runtime

# Check deployment status
kubectl get inferenceservice granite-model -o jsonpath='{.status.conditions[?(@.type=="Ready")].status}'
```

### Filter Model Deployments

```bash
# Filter by label
kubectl get inferenceservices -l environment=production

# Filter by authentication status
kubectl get inferenceservices -o json | \
  jq '.items[] | select(.metadata.annotations["security.opendatahub.io/enable-auth"] == "true") | .metadata.name'

# List deployments using specific runtime
RUNTIME="granite-runtime"
kubectl get inferenceservices -o json | \
  jq --arg runtime "$RUNTIME" '.items[] | select(.spec.predictor.model.runtime == $runtime) | .metadata.name'

# List GPU-enabled deployments
kubectl get inferenceservices -o json | \
  jq '.items[] | select(.spec.predictor.model.resources.requests["nvidia.com/gpu"] != null) | .metadata.name'
```

## Updating Model Deployments

Model deployments can be updated to change models, adjust resources, or modify configurations. Updates should be done carefully to minimize downtime.

### Update Model Version

```bash
# Update to a new model version
kubectl patch inferenceservice granite-model --type='json' -p='[
  {"op": "replace", "path": "/spec/predictor/model/storageUri", 
   "value": "oci://registry.redhat.io/rhelai1/modelcar-granite-3-1-8b-instruct:1.6"}
]'

# Or using kubectl set (if supported)
kubectl set env inferenceservice/granite-model \
  STORAGE_URI=oci://registry.redhat.io/rhelai1/modelcar-granite-3-1-8b-instruct:1.6
```

### Scale Model Deployment

```bash
# Update replica count
kubectl patch inferenceservice granite-model --type='merge' -p='
{
  "spec": {
    "predictor": {
      "minReplicas": 2,
      "maxReplicas": 5
    }
  }
}'

# Enable autoscaling
kubectl patch inferenceservice granite-model --type='merge' -p='
{
  "spec": {
    "predictor": {
      "minReplicas": 1,
      "maxReplicas": 10,
      "scaleTarget": 80,
      "scaleMetric": "cpu"
    }
  }
}'
```

### Update Resource Allocations

```bash
# Increase memory and CPU
kubectl patch inferenceservice granite-model --type='merge' -p='
{
  "spec": {
    "predictor": {
      "model": {
        "resources": {
          "requests": {
            "cpu": "4",
            "memory": "32Gi"
          },
          "limits": {
            "cpu": "8",
            "memory": "48Gi"
          }
        }
      }
    }
  }
}'
```

### Update ServingRuntime Configuration

```bash
# Update runtime arguments
kubectl patch servingruntime granite-runtime --type='json' -p='[
  {"op": "replace", "path": "/spec/containers/0/args", 
   "value": ["--port=8080", "--model=/mnt/models", "--tensor-parallel-size=2", "--max-model-len=8192"]}
]'

# Update container image
kubectl patch servingruntime granite-runtime --type='merge' -p='
{
  "spec": {
    "containers": [{
      "name": "kserve-container",
      "image": "quay.io/modh/vllm:rhoai-2.21-cuda"
    }]
  }
}'
```

### Enable/Disable Authentication

```bash
# Enable authentication
kubectl annotate inferenceservice granite-model \
  security.opendatahub.io/enable-auth=true --overwrite

# Disable authentication
kubectl annotate inferenceservice granite-model \
  security.opendatahub.io/enable-auth=false --overwrite
```

### Update Route Configuration

```bash
# Update TLS configuration
kubectl patch route granite-model --type='merge' -p='
{
  "spec": {
    "tls": {
      "termination": "reencrypt",
      "certificate": "-----BEGIN CERTIFICATE-----\n<YOUR_CERTIFICATE_CONTENT>\n-----END CERTIFICATE-----",
      "key": "-----BEGIN PRIVATE KEY-----\n<YOUR_PRIVATE_KEY_CONTENT>\n-----END PRIVATE KEY-----"
    }
  }
}'
```

## Deleting Model Deployments

When deleting model deployments, remove resources in the reverse order of creation to ensure clean cleanup.

### Basic Deletion

```bash
# Delete in reverse order: Route → InferenceService → ServingRuntime
kubectl delete route granite-model
kubectl delete inferenceservice granite-model
kubectl delete servingruntime granite-runtime
```

### Delete All Resources for a Model

```bash
# Delete all resources with a specific label
MODEL_NAME="granite-model"
kubectl delete route,inferenceservice,servingruntime \
  -l serving.kserve.io/inferenceservice=$MODEL_NAME

# Force deletion if stuck
kubectl delete inferenceservice granite-model --force --grace-period=0
```

### Important Notes on Deletion

⚠️ **Warning**: Deleting an InferenceService will:
- Remove all associated pods and services
- Delete any in-memory model cache
- Terminate active inference requests
- Remove autoscaling configurations

### Cleanup Verification

```bash
# Verify all resources are deleted
kubectl get all -l serving.kserve.io/inferenceservice=granite-model

# Check for lingering PVCs
kubectl get pvc -l serving.kserve.io/inferenceservice=granite-model

# Check for finalizers preventing deletion
kubectl get inferenceservice granite-model -o jsonpath='{.metadata.finalizers}'
```

## Practical Examples

This section shows progressively more advanced model serving configurations, each building on the previous example with additional functionality.

### Example 1: Basic/Minimal Configuration

The simplest possible deployment with just the essentials:

```yaml
# minimal-deployment.yaml
apiVersion: v1
kind: List
items:
  # Minimal ServingRuntime - bare minimum fields
  - apiVersion: serving.kserve.io/v1alpha1
    kind: ServingRuntime
    metadata:
      name: minimal-model  # Must match InferenceService name
    spec:
      containers:
        - name: kserve-container
          image: 'quay.io/modh/vllm:rhoai-2.20-cuda'
          args:
            - '--port=8080'       # Required: serving port
            - '--model=/mnt/models'  # Required: model mount path
      supportedModelFormats:
        - name: vLLM
          autoSelect: true  # Automatically select this runtime for vLLM models
  
  # Minimal InferenceService - only required fields
  - apiVersion: serving.kserve.io/v1beta1
    kind: InferenceService
    metadata:
      name: minimal-model
    spec:
      predictor:
        model:
          modelFormat:
            name: vLLM  # Must match supportedModelFormats in runtime
          runtime: minimal-model  # Must match ServingRuntime name
          storageUri: 'oci://registry.redhat.io/rhelai1/modelcar-granite-3-1-8b-instruct:1.5'  # Model location
```

Deploy and test:
```bash
kubectl apply -f minimal-deployment.yaml
kubectl get inferenceservice minimal-model -w
```

### Example 2: Adding Custom vLLM Arguments

Build on the basic configuration by adding vLLM-specific arguments to optimize performance:

```yaml
# custom-args-deployment.yaml
apiVersion: v1
kind: List
items:
  - apiVersion: serving.kserve.io/v1alpha1
    kind: ServingRuntime
    metadata:
      name: custom-args-model  # Must match InferenceService name
      labels:
        opendatahub.io/dashboard: 'true'  # Show in dashboard
    spec:
      containers:
        - name: kserve-container
          image: 'quay.io/modh/vllm:rhoai-2.20-cuda'
          args:
            - '--port=8080'
            - '--model=/mnt/models'
            - '--served-model-name={{.Name}}'  # Uses InferenceService name
            # vLLM performance tuning arguments
            - '--max-model-len=4096'         # Limit context window (saves memory)
            - '--max-num-seqs=128'           # Max concurrent requests
            - '--gpu-memory-utilization=0.9' # Use 90% of GPU memory
          resources:  # Resource configuration for GPU
            requests:
              memory: 16Gi
            limits:
              memory: 24Gi
              nvidia.com/gpu: '1'  # GPU allocation
      supportedModelFormats:
        - name: vLLM
          autoSelect: true
  
  - apiVersion: serving.kserve.io/v1beta1
    kind: InferenceService
    metadata:
      name: custom-args-model
      labels:
        opendatahub.io/dashboard: 'true'
    spec:
      predictor:
        model:
          modelFormat:
            name: vLLM
          runtime: custom-args-model  # Must match InferenceService name
          storageUri: 'oci://registry.redhat.io/rhelai1/modelcar-granite-3-1-8b-instruct:1.5'
          resources:  # Match or exceed runtime resources
            requests:
              cpu: '2'
              memory: 16Gi
              nvidia.com/gpu: '1'
            limits:
              cpu: '4'
              memory: 24Gi
              nvidia.com/gpu: '1'
```

### Example 3: Adding Authentication

Add OpenShift AI authentication to secure your model endpoint:

```yaml
# auth-deployment.yaml
apiVersion: v1
kind: List
items:
  - apiVersion: serving.kserve.io/v1alpha1
    kind: ServingRuntime
    metadata:
      name: auth-model  # Must match InferenceService name
      labels:
        opendatahub.io/dashboard: 'true'
    spec:
      containers:
        - name: kserve-container
          image: 'quay.io/modh/vllm:rhoai-2.20-cuda'
          args:
            - '--port=8080'
            - '--model=/mnt/models'
            - '--max-model-len=4096'
          resources:
            limits:
              memory: 24Gi
              nvidia.com/gpu: '1'
      supportedModelFormats:
        - name: vLLM
          autoSelect: true
  
  - apiVersion: serving.kserve.io/v1beta1
    kind: InferenceService
    metadata:
      name: auth-model
      annotations:
        # Authentication-specific annotations
        security.opendatahub.io/enable-auth: 'true'  # Enable OpenShift AI auth
        serving.kserve.io/deploymentMode: RawDeployment  # Required for auth
      labels:
        opendatahub.io/dashboard: 'true'
    spec:
      predictor:
        model:
          modelFormat:
            name: vLLM
          runtime: auth-model  # Must match InferenceService name
          storageUri: 'oci://registry.redhat.io/rhelai1/modelcar-granite-3-1-8b-instruct:1.5'
          resources:
            requests:
              cpu: '2'
              memory: 16Gi
              nvidia.com/gpu: '1'
            limits:
              cpu: '4'
              memory: 24Gi
              nvidia.com/gpu: '1'
        # GPU scheduling configuration
        tolerations:
          - effect: NoSchedule
            key: nvidia.com/gpu
            operator: Exists
```

### Example 4: Exposing with a Route

Add external access to your model through an OpenShift Route:

```yaml
# route-deployment.yaml
apiVersion: v1
kind: List
items:
  - apiVersion: serving.kserve.io/v1alpha1
    kind: ServingRuntime
    metadata:
      name: route-model  # Must match InferenceService name
      labels:
        opendatahub.io/dashboard: 'true'
    spec:
      containers:
        - name: kserve-container
          image: 'quay.io/modh/vllm:rhoai-2.20-cuda'
          args:
            - '--port=8080'
            - '--model=/mnt/models'
            - '--max-model-len=4096'
          resources:
            limits:
              memory: 24Gi
              nvidia.com/gpu: '1'
      supportedModelFormats:
        - name: vLLM
          autoSelect: true
  
  - apiVersion: serving.kserve.io/v1beta1
    kind: InferenceService
    metadata:
      name: route-model
      annotations:
        security.opendatahub.io/enable-auth: 'true'
        serving.kserve.io/deploymentMode: RawDeployment
      labels:
        # Critical label for route exposure
        networking.kserve.io/visibility: exposed  # Enables external access
        opendatahub.io/dashboard: 'true'
    spec:
      predictor:
        model:
          modelFormat:
            name: vLLM
          runtime: route-model  # Must match InferenceService name
          storageUri: 'oci://registry.redhat.io/rhelai1/modelcar-granite-3-1-8b-instruct:1.5'
          resources:
            requests:
              cpu: '2'
              memory: 16Gi
              nvidia.com/gpu: '1'
            limits:
              cpu: '4'
              memory: 24Gi
              nvidia.com/gpu: '1'
        tolerations:
          - effect: NoSchedule
            key: nvidia.com/gpu
            operator: Exists
  
  # Route for external HTTPS access
  - apiVersion: route.openshift.io/v1
    kind: Route
    metadata:
      name: route-model
      labels:
        inferenceservice-name: route-model  # Links to InferenceService
    spec:
      to:
        kind: Service
        name: route-model-predictor  # Auto-created service name
        weight: 100
      port:
        targetPort: http
      tls:  # TLS configuration for secure access
        termination: edge  # TLS terminated at router
        insecureEdgeTerminationPolicy: Redirect  # Force HTTPS
      wildcardPolicy: None
```

### Example 5: Multiple GPUs on a Single Node

Configure a model to use multiple GPUs with tensor parallelism:

```yaml
# multi-gpu-deployment.yaml
apiVersion: v1
kind: List
items:
  - apiVersion: serving.kserve.io/v1alpha1
    kind: ServingRuntime
    metadata:
      name: multi-gpu-model  # Must match InferenceService name
      annotations:
        opendatahub.io/accelerator-name: nvidia-a100  # Target specific GPU type
      labels:
        opendatahub.io/dashboard: 'true'
    spec:
      containers:
        - name: kserve-container
          image: 'quay.io/modh/vllm:rhoai-2.20-cuda'
          args:
            - '--port=8080'
            - '--model=/mnt/models'
            # Multi-GPU specific configurations
            - '--tensor-parallel-size=2'     # Split model across 2 GPUs
            - '--distributed-executor-backend=mp'  # Required for multi-GPU
            - '--max-model-len=8192'         # Larger context enabled by more memory
          env:
            - name: VLLM_ATTENTION_BACKEND
              value: FLASHINFER  # Performance optimization for large models
          volumeMounts:
            - mountPath: /dev/shm  # Shared memory critical for multi-GPU
              name: shm
          resources:
            limits:
              memory: 96Gi
              nvidia.com/gpu: '2'  # Must match tensor-parallel-size
      supportedModelFormats:
        - name: vLLM
          autoSelect: true
      volumes:
        - name: shm  # Larger shared memory for multi-GPU communication
          emptyDir:
            medium: Memory
            sizeLimit: 16Gi
  
  - apiVersion: serving.kserve.io/v1beta1
    kind: InferenceService
    metadata:
      name: multi-gpu-model
      annotations:
        security.opendatahub.io/enable-auth: 'true'
        serving.kserve.io/deploymentMode: RawDeployment
        serving.kserve.io/enable-prometheus-scraping: 'true'  # Monitor multi-GPU usage
      labels:
        networking.kserve.io/visibility: exposed
        opendatahub.io/dashboard: 'true'
    spec:
      predictor:
        # Autoscaling configuration for production
        minReplicas: 1
        maxReplicas: 3       # Scale horizontally across nodes
        scaleTarget: 80      # CPU utilization target
        scaleMetric: cpu
        model:
          modelFormat:
            name: vLLM
          runtime: multi-gpu-model  # Must match InferenceService name
          # Large model requiring multiple GPUs
          storageUri: 'oci://registry.redhat.io/rhelai1/modelcar-llama-3-3-70b-instruct:1.5'
          resources:
            requests:
              cpu: '8'
              memory: 80Gi
              nvidia.com/gpu: '2'  # Must match runtime GPU count
            limits:
              cpu: '16'
              memory: 96Gi
              nvidia.com/gpu: '2'  # Ensure same GPU count as runtime
        tolerations:
          - effect: NoSchedule
            key: nvidia.com/gpu
            operator: Exists
          - effect: NoSchedule               # A100-specific toleration
            key: nvidia.com/gpu-model
            operator: Equal
            value: A100
  
  - apiVersion: route.openshift.io/v1
    kind: Route
    metadata:
      name: multi-gpu-model
    spec:
      to:
        kind: Service
        name: multi-gpu-model-predictor
      tls:
        termination: edge
        insecureEdgeTerminationPolicy: Redirect
```

### Deployment Commands for All Examples

```bash
# Deploy each example progressively
kubectl apply -f minimal-deployment.yaml
kubectl apply -f custom-args-deployment.yaml
kubectl apply -f auth-deployment.yaml
kubectl apply -f route-deployment.yaml
kubectl apply -f multi-gpu-deployment.yaml

# Monitor deployment status
kubectl get inferenceservice -w

# Test endpoints (internal)
kubectl port-forward service/<model-name>-predictor 8080:80
curl http://localhost:8080/v2/models

# Test external routes
ROUTE_URL=$(kubectl get route <model-name> -o jsonpath='{.spec.host}')
curl https://$ROUTE_URL/v2/models
```

## Verification and Troubleshooting

### Verify Deployment Status

```bash
# Check InferenceService readiness
kubectl get inferenceservice <model-name> -o jsonpath='{.status.conditions[?(@.type=="Ready")]}'

# Watch deployment progress
kubectl get inferenceservice <model-name> -w

# Check all components
kubectl get pods,services,deployments -l serving.kserve.io/inferenceservice=<model-name>
```

### View Logs

```bash
# View predictor pod logs
kubectl logs -l serving.kserve.io/inferenceservice=<model-name> -c kserve-container

# Stream logs
kubectl logs -f -l serving.kserve.io/inferenceservice=<model-name>

# View previous container logs (if crashed)
kubectl logs -l serving.kserve.io/inferenceservice=<model-name> -c kserve-container --previous
```

### Common Issues and Solutions

#### Model Not Loading

```bash
# Check pod events
kubectl describe pod -l serving.kserve.io/inferenceservice=<model-name>

# Common causes:
# - Insufficient memory: Increase memory limits
# - Wrong model format: Verify storageUri and model compatibility
# - GPU not available: Check node labels and tolerations
```

#### Authentication Errors

```bash
# Verify auth annotation
kubectl get inferenceservice <model-name> -o jsonpath='{.metadata.annotations.security\.opendatahub\.io/enable-auth}'

# Check service account
kubectl get sa -l serving.kserve.io/inferenceservice=<model-name>

# Verify RBAC
kubectl auth can-i --list --as=system:serviceaccount:<namespace>:<sa-name>
```

#### Route Not Accessible

```bash
# Check route status
kubectl get route <model-name> -o jsonpath='{.status.ingress[0].conditions[?(@.type=="Admitted")]}'

# Verify service exists
kubectl get service <model-name>-predictor

# Test internal connectivity
kubectl run test-curl --image=curlimages/curl:latest --rm -it -- \
  curl http://<model-name>-predictor.<namespace>.svc.cluster.local:80/v1/models
```

#### GPU Allocation Issues

```bash
# Check node GPU availability
kubectl get nodes -o custom-columns=NAME:.metadata.name,GPUs:.status.capacity.nvidia\\.com/gpu

# Verify pod GPU requests
kubectl get pod -l serving.kserve.io/inferenceservice=<model-name> -o jsonpath='{.items[0].spec.containers[0].resources}'

# Check GPU operator status
kubectl get pods -n nvidia-gpu-operator
```

### Performance Troubleshooting

```bash
# Check resource usage
kubectl top pod -l serving.kserve.io/inferenceservice=<model-name>

# View HPA status (if autoscaling enabled)
kubectl get hpa

# Check response times
kubectl exec -it <pod-name> -- curl -w "@curl-format.txt" -o /dev/null -s http://localhost:8080/v1/models
```

## Best Practices

### Naming Conventions

- Use consistent names for ServingRuntime and InferenceService (e.g., both named `granite-model`)
- Include model version in names for easy identification (e.g., `llama-70b-v1`)
- Use descriptive prefixes for environment separation (e.g., `prod-`, `dev-`, `test-`)

### Resource Allocation

- **Development**: Start with minimal resources and scale up as needed
- **Production**: 
  - Set resource requests to guarantee minimum performance
  - Set resource limits 20-30% above requests for burst capacity
  - Use GPU only when necessary (some models run efficiently on CPU)

### Security Considerations

- **Always enable authentication** for production deployments:
  ```yaml
  security.opendatahub.io/enable-auth: 'true'
  ```
- Use TLS termination at the route level
- Implement network policies to restrict access
- Regularly update serving runtime images

### Model Selection Guidelines

- Choose quantized models (FP8, W8A8) for better GPU memory efficiency
- Use tensor parallelism for models larger than single GPU memory
- Consider model size vs. accuracy trade-offs

### Production Deployment Checklist

- [ ] Enable authentication
- [ ] Configure autoscaling
- [ ] Set appropriate resource requests/limits
- [ ] Add health checks and readiness probes
- [ ] Configure monitoring and alerts
- [ ] Document model version and parameters
- [ ] Test rollback procedures
- [ ] Verify GPU node affinity and tolerations

## Field Reference

### ServingRuntime Fields

| Field | Type | Required | Description | Example |
|-------|------|----------|-------------|---------|
| `metadata.name` | string | Yes | Unique runtime identifier | `vllm-runtime` |
| `metadata.annotations.opendatahub.io/accelerator-name` | string | No | Accelerator profile reference | `nvidia-a100` |
| `metadata.annotations.opendatahub.io/recommended-accelerators` | JSON array | No | List of compatible GPU types | `["nvidia.com/gpu"]` |
| `metadata.labels.opendatahub.io/dashboard` | string | No | Show in OpenShift AI dashboard | `'true'` |
| `spec.containers[].name` | string | Yes | Container name | `kserve-container` |
| `spec.containers[].image` | string | Yes | Serving container image | `quay.io/modh/vllm:rhoai-2.20-cuda` |
| `spec.containers[].args` | array | No | Container arguments | `["--port=8080"]` |
| `spec.containers[].env` | array | No | Environment variables | See examples |
| `spec.supportedModelFormats[].name` | string | Yes | Model format name | `vLLM` |
| `spec.supportedModelFormats[].autoSelect` | boolean | No | Auto-select this format | `true` |
| `spec.volumes` | array | No | Volume definitions | See examples |

### InferenceService Fields

| Field | Type | Required | Description | Example |
|-------|------|----------|-------------|---------|
| `metadata.name` | string | Yes | Unique model identifier | `granite-model` |
| `metadata.annotations.security.opendatahub.io/enable-auth` | string | No | Enable authentication | `'true'` or `'false'` |
| `metadata.annotations.serving.kserve.io/deploymentMode` | string | No | Deployment mode | `RawDeployment` |
| `metadata.labels.networking.kserve.io/visibility` | string | No | Network visibility | `exposed` |
| `spec.predictor.minReplicas` | integer | No | Minimum pod replicas | `1` |
| `spec.predictor.maxReplicas` | integer | No | Maximum pod replicas | `5` |
| `spec.predictor.scaleTarget` | integer | No | Autoscaling target percentage | `80` |
| `spec.predictor.scaleMetric` | string | No | Autoscaling metric | `cpu` or `memory` |
| `spec.predictor.model.modelFormat.name` | string | Yes | Model format | `vLLM` |
| `spec.predictor.model.runtime` | string | Yes | ServingRuntime reference | `vllm-runtime` |
| `spec.predictor.model.storageUri` | string | Yes | Model location | `oci://registry.redhat.io/...` |
| `spec.predictor.model.args` | array | No | Model server arguments | `["--max-model-len=4096"]` |
| `spec.predictor.model.resources` | object | No | Resource requirements | See examples |
| `spec.predictor.tolerations` | array | No | Node scheduling tolerations | See examples |

### Common Annotations

| Annotation | Resource | Description | Values |
|------------|----------|-------------|--------|
| `opendatahub.io/dashboard` | Both | Display in OpenShift AI dashboard | `'true'` |
| `openshift.io/display-name` | Both | Human-readable name | Any string |
| `security.opendatahub.io/enable-auth` | InferenceService | Enable authentication | `'true'`, `'false'` |
| `serving.kserve.io/deploymentMode` | InferenceService | Deployment strategy | `RawDeployment`, `Serverless` |
| `serving.kserve.io/enable-prometheus-scraping` | InferenceService | Enable metrics | `'true'` |
| `prometheus.io/scrape` | ServingRuntime | Enable Prometheus scraping | `'true'` |
| `prometheus.io/port` | ServingRuntime | Metrics port | `'8080'` |
| `prometheus.io/path` | ServingRuntime | Metrics endpoint | `/metrics` |

## Using with Kubernetes MCP Server

The Kubernetes MCP server provides programmatic access to manage model serving resources. Below are the tool mappings and usage patterns.

### MCP Tool Mapping

| Operation | MCP Tool | Description |
|-----------|----------|-------------|
| Create resources | `mcp__Kubernetes__resources_create_or_update` | Create ServingRuntime, InferenceService, Route |
| List resources | `mcp__Kubernetes__resources_list` | List model deployments |
| Get specific resource | `mcp__Kubernetes__resources_get` | View details of specific resources |
| Update resources | `mcp__Kubernetes__resources_create_or_update` | Update existing resources |
| Delete resources | `mcp__Kubernetes__resources_delete` | Remove model deployments |
| View logs | `mcp__Kubernetes__pods_log` | Check model server logs |
| Execute commands | `mcp__Kubernetes__pods_exec` | Run commands in pods |

### Creating Model Deployments with MCP

```javascript
// Create ServingRuntime
await mcp__Kubernetes__resources_create_or_update({
  resource: JSON.stringify({
    apiVersion: "serving.kserve.io/v1alpha1",
    kind: "ServingRuntime",
    metadata: {
      name: "mcp-model-runtime",
      namespace: "default"
    },
    spec: {
      containers: [{
        name: "kserve-container",
        image: "quay.io/modh/vllm:rhoai-2.20-cuda",
        args: ["--port=8080", "--model=/mnt/models"]
      }],
      supportedModelFormats: [{
        name: "vLLM",
        autoSelect: true
      }]
    }
  })
});

// Create InferenceService
await mcp__Kubernetes__resources_create_or_update({
  resource: JSON.stringify({
    apiVersion: "serving.kserve.io/v1beta1",
    kind: "InferenceService",
    metadata: {
      name: "mcp-model",
      namespace: "default"
    },
    spec: {
      predictor: {
        model: {
          modelFormat: { name: "vLLM" },
          runtime: "mcp-model-runtime",
          storageUri: "oci://registry.redhat.io/rhelai1/modelcar-granite-3-1-8b-instruct:1.5"
        }
      }
    }
  })
});
```

### Listing Model Deployments with MCP

```javascript
// List all InferenceServices
const inferenceServices = await mcp__Kubernetes__resources_list({
  apiVersion: "serving.kserve.io/v1beta1",
  kind: "InferenceService",
  namespace: "default"
});

// List all ServingRuntimes
const servingRuntimes = await mcp__Kubernetes__resources_list({
  apiVersion: "serving.kserve.io/v1alpha1",
  kind: "ServingRuntime",
  namespace: "default"
});
```

### Monitoring with MCP

```javascript
// Get model deployment status
const model = await mcp__Kubernetes__resources_get({
  apiVersion: "serving.kserve.io/v1beta1",
  kind: "InferenceService",
  name: "mcp-model",
  namespace: "default"
});

// View model server logs
const pods = await mcp__Kubernetes__pods_list_in_namespace({
  namespace: "default",
  labelSelector: "serving.kserve.io/inferenceservice=mcp-model"
});

if (pods.items.length > 0) {
  const logs = await mcp__Kubernetes__pods_log({
    name: pods.items[0].metadata.name,
    namespace: "default",
    container: "kserve-container"
  });
}
```

### MCP Limitations

- Cannot use `kubectl apply` with file paths - must provide full resource JSON
- No support for `kubectl patch` - use full resource updates
- Cannot use shell redirections or pipes
- Limited support for complex label selectors

### Best Practices for MCP

1. Always specify namespace explicitly
2. Use JSON.stringify() for resource specifications
3. Check resource existence before updates
4. Handle API version compatibility
5. Implement proper error handling for failed operations

## Appendix: Pre-built ModelCars

The following table lists all available pre-built ModelCars from Red Hat's validated models collection. These models are optimized for deployment on OpenShift AI and can be used with the OCI registry storage method by specifying the URI in the `storageUri` field of your InferenceService configuration.

| Model ID | ModelCar URI |
|----------|-------------|
| `RedHatAI/Llama-4-Scout-17B-16E-Instruct-FP8-dynamic` | `oci://registry.redhat.io/rhelai1/modelcar-llama-4-scout-17b-16e-instruct-fp8-dynamic:1.5` |
| `RedHatAI/Llama-4-Scout-17B-16E-Instruct-quantized.w4a16` | `oci://registry.redhat.io/rhelai1/modelcar-llama-4-scout-17b-16e-instruct-quantized-w4a16:1.5` |
| `RedHatAI/Llama-4-Scout-17B-16E-Instruct` | `oci://registry.redhat.io/rhelai1/modelcar-llama-4-scout-17b-16e-instruct:1.5` |
| `RedHatAI/Llama-4-Maverick-17B-128E-Instruct` | `oci://registry.redhat.io/rhelai1/modelcar-llama-4-maverick-17b-128e-instruct:1.5` |
| `RedHatAI/Llama-4-Maverick-17B-128E-Instruct-FP8` | `oci://registry.redhat.io/rhelai1/modelcar-llama-4-maverick-17b-128e-instruct-fp8:1.5` |
| `RedHatAI/Mistral-Small-3.1-24B-Instruct-2503-FP8-dynamic` | `oci://registry.redhat.io/rhelai1/modelcar-mistral-small-3-1-24b-instruct-2503-fp8-dynamic:1.5` |
| `RedHatAI/Mistral-Small-3.1-24B-Instruct-2503-quantized.w8a8` | `oci://registry.redhat.io/rhelai1/modelcar-mistral-small-3-1-24b-instruct-2503-quantized-w8a8:1.5` |
| `RedHatAI/Mistral-Small-3.1-24B-Instruct-2503-quantized.w4a16` | `oci://registry.redhat.io/rhelai1/modelcar-mistral-small-3-1-24b-instruct-2503-quantized-w4a16:1.5` |
| `RedHatAI/Mistral-Small-3.1-24B-Instruct-2503` | `oci://registry.redhat.io/rhelai1/modelcar-mistral-small-3-1-24b-instruct-2503:1.5` |
| `RedHatAI/Mistral-Small-24B-Instruct-2501-FP8-dynamic` | `oci://registry.redhat.io/rhelai1/modelcar-mistral-small-24b-instruct-2501-fp8-dynamic:1.5` |
| `RedHatAI/Mistral-Small-24B-Instruct-2501-quantized.w8a8` | `oci://registry.redhat.io/rhelai1/modelcar-mistral-small-24b-instruct-2501-quantized-w8a8:1.5` |
| `RedHatAI/Mistral-Small-24B-Instruct-2501` | `oci://registry.redhat.io/rhelai1/modelcar-mistral-small-24b-instruct-2501:1.5` |
| `RedHatAI/phi-4` | `oci://registry.redhat.io/rhelai1/modelcar-phi-4:1.5` |
| `RedHatAI/phi-4-quantized.w4a16` | `oci://registry.redhat.io/rhelai1/modelcar-phi-4-quantized-w4a16:1.5` |
| `RedHatAI/phi-4-quantized.w8a8` | `oci://registry.redhat.io/rhelai1/modelcar-phi-4-quantized-w8a8:1.5` |
| `RedHatAI/phi-4-FP8-dynamic` | `oci://registry.redhat.io/rhelai1/modelcar-phi-4-fp8-dynamic:1.5` |
| `RedHatAI/Llama-3.3-70B-Instruct-FP8-dynamic` | `oci://registry.redhat.io/rhelai1/modelcar-llama-3-3-70b-instruct-fp8-dynamic:1.5` |
| `RedHatAI/Llama-3.3-70B-Instruct` | `oci://registry.redhat.io/rhelai1/modelcar-llama-3-3-70b-instruct:1.5` |
| `RedHatAI/granite-3.1-8b-instruct-FP8-dynamic` | `oci://registry.redhat.io/rhelai1/modelcar-granite-3-1-8b-instruct-fp8-dynamic:1.5` |
| `RedHatAI/granite-3.1-8b-instruct` | `oci://registry.redhat.io/rhelai1/modelcar-granite-3-1-8b-instruct:1.5` |
| `RedHatAI/Qwen2.5-7B-Instruct-FP8-dynamic` | `oci://registry.redhat.io/rhelai1/modelcar-qwen2-5-7b-instruct-fp8-dynamic:1.5` |
| `RedHatAI/Qwen2.5-7B-Instruct` | `oci://registry.redhat.io/rhelai1/modelcar-qwen2-5-7b-instruct:1.5` |
| `RedHatAI/Llama-3.1-8B-Instruct` | `oci://registry.redhat.io/rhelai1/modelcar-llama-3-1-8b-instruct:1.5` |
| `RedHatAI/Llama-3.1-Nemotron-70B-Instruct-HF-FP8-dynamic` | `oci://registry.redhat.io/rhelai1/modelcar-llama-3-1-nemotron-70b-instruct-hf-fp8-dynamic:1.5` |
| `RedHatAI/Mixtral-8x7B-Instruct-v0.1` | `oci://registry.redhat.io/rhelai1/modelcar-mixtral-8x7b-instruct-v0-1:1.4` |
| `RedHatAI/Llama-3.1-Nemotron-70B-Instruct-HF` | `oci://registry.redhat.io/rhelai1/modelcar-llama-3-1-nemotron-70b-instruct-hf:1.5` |
| `RedHatAI/Llama-3.3-70B-Instruct-quantized.w8a8` | `oci://registry.redhat.io/rhelai1/modelcar-llama-3-3-70b-instruct-quantized-w8a8:1.5` |
| `RedHatAI/Llama-3.3-70B-Instruct-quantized.w4a16` | `oci://registry.redhat.io/rhelai1/modelcar-llama-3-3-70b-instruct-quantized-w4a16:1.5` |
| `RedHatAI/granite-3.1-8b-instruct-quantized.w8a8` | `oci://registry.redhat.io/rhelai1/modelcar-granite-3-1-8b-instruct-quantized-w8a8:1.5` |
| `RedHatAI/granite-3.1-8b-instruct-quantized.w4a16` | `oci://registry.redhat.io/rhelai1/modelcar-granite-3-1-8b-instruct-quantized-w4a16:1.5` |
| `RedHatAI/Qwen2.5-7B-Instruct-quantized.w8a8` | `oci://registry.redhat.io/rhelai1/modelcar-qwen2-5-7b-instruct-quantized-w8a8:1.5` |
| `RedHatAI/Qwen2.5-7B-Instruct-quantized.w4a16` | `oci://registry.redhat.io/rhelai1/modelcar-qwen2-5-7b-instruct-quantized-w4a16:1.5` |
| `RedHatAI/granite-3.1-8b-base-quantized.w4a16` | `oci://registry.redhat.io/rhelai1/modelcar-granite-3-1-8b-base-quantized-w4a16:1.5` |
| `RedHatAI/Mistral-Small-24B-Instruct-2501-quantized.w4a16` | `oci://registry.redhat.io/rhelai1/modelcar-mistral-small-24b-instruct-2501-quantized-w4a16:1.5` |
| `RedHatAI/Meta-Llama-3.1-8B-Instruct-quantized.w8a8` | `oci://registry.redhat.io/rhelai1/modelcar-llama-3-1-8b-instruct-quantized-w8a8:1.5` |
| `RedHatAI/Meta-Llama-3.1-8B-Instruct-FP8-dynamic` | `oci://registry.redhat.io/rhelai1/modelcar-llama-3-1-8b-instruct-fp8-dynamic:1.5` |
| `RedHatAI/Meta-Llama-3.1-8B-Instruct-quantized.w4a16` | `oci://registry.redhat.io/rhelai1/modelcar-llama-3-1-8b-instruct-quantized-w4a16:1.5` |
| `RedHatAI/gemma-2-9b-it-FP8` | `oci://registry.redhat.io/rhelai1/modelcar-gemma-2-9b-it-FP8:1.5` |
| `RedHatAI/gemma-2-9b-it` | `oci://registry.redhat.io/rhelai1/modelcar-gemma-2-9b-it:1.5` |

### Usage Notes

- **Model Selection**: Choose models based on your hardware constraints and performance requirements
- **Quantization**: Models with `FP8-dynamic`, `w4a16`, or `w8a8` quantization require less GPU memory
- **GPU Requirements**: Larger models (70B parameters) require more GPU memory and compute resources
- **Version**: Most models use version `1.5`, except `Mixtral-8x7B-Instruct-v0.1` which uses `1.4`
- **Registry**: All models are hosted in the Red Hat registry at `registry.redhat.io/rhelai1/`

### Example Usage

To use any of these models in your InferenceService, simply replace the `storageUri` field:

```yaml
spec:
  predictor:
    model:
      storageUri: 'oci://registry.redhat.io/rhelai1/modelcar-granite-3-1-8b-instruct:1.5'
```

For more information about each model, visit the [Red Hat AI validated models collection](https://huggingface.co/collections/RedHatAI/red-hat-ai-validated-models-v10-682613dc19c4a596dbac9437) on Hugging Face.

# OpenShift AI API Documentation


# Data Science Projects

Data Science Projects in Red Hat OpenShift AI provide isolated environments for organizing your machine learning work. These projects are OpenShift projects (Kubernetes namespaces) with specific labels and annotations that enable integration with the OpenShift AI dashboard and features.

## Overview

A data science project is essentially an OpenShift project with the label `opendatahub.io/dashboard: 'true'`. This label makes the project visible in the OpenShift AI dashboard and enables AI/ML-specific features like:
- Workbench creation (Jupyter notebooks)
- Data connections
- Model serving
- Pipeline management
- Persistent storage

## Creating Projects

### Method 1: Declarative (Using YAML)

The declarative approach uses YAML files to define the desired state of your project. This method is recommended for:
- Version control and GitOps workflows
- Reproducible deployments
- Automated provisioning

#### Basic Project

```yaml
# project-basic.yaml
apiVersion: project.openshift.io/v1
kind: Project
metadata:
  name: my-ds-project
  labels:
    # Required: Makes project visible in OpenShift AI dashboard
    opendatahub.io/dashboard: 'true'
    # Automatically added: Matches the project name
    kubernetes.io/metadata.name: my-ds-project
  annotations:
    # Human-readable display name shown in the dashboard
    openshift.io/display-name: My Data Science Project
    # Optional: Project description
    openshift.io/description: 'Project for machine learning experiments'
spec: {}
```

Apply the project:
```bash
kubectl apply -f project-basic.yaml
```

#### Standard Project with Common Annotations

```yaml
# project-standard.yaml
apiVersion: project.openshift.io/v1
kind: Project
metadata:
  name: ml-fraud-detection
  labels:
    # Required for OpenShift AI
    opendatahub.io/dashboard: 'true'
    kubernetes.io/metadata.name: ml-fraud-detection
    # Optional: Custom labels for organization
    team: data-science
    environment: development
    project-type: ml-experiment
  annotations:
    openshift.io/display-name: Fraud Detection ML
    openshift.io/description: 'Machine learning models for credit card fraud detection'
    # Optional: Who requested/owns the project
    openshift.io/requester: john.doe@example.com
    # Optional: Project documentation link
    project.docs.url: 'https://wiki.example.com/fraud-detection'
spec: {}
```

#### Advanced Project with Resource Quotas

```yaml
# project-advanced.yaml
apiVersion: v1
kind: List
items:
  # The Project
  - apiVersion: project.openshift.io/v1
    kind: Project
    metadata:
      name: production-ml-models
      labels:
        opendatahub.io/dashboard: 'true'
        kubernetes.io/metadata.name: production-ml-models
        environment: production
        compliance: pci-dss
      annotations:
        openshift.io/display-name: Production ML Models
        openshift.io/description: 'Production-ready ML models with resource limits'
        openshift.io/requester: ml-ops-team@example.com
    spec: {}
  
  # Resource Quota (applied after project creation)
  - apiVersion: v1
    kind: ResourceQuota
    metadata:
      name: compute-quota
      namespace: production-ml-models
    spec:
      hard:
        requests.cpu: "100"
        requests.memory: 200Gi
        requests.storage: 1Ti
        persistentvolumeclaims: "10"
        pods: "50"
        requests.nvidia.com/gpu: "4"
```

### Method 2: Imperative (Using Commands)

The imperative approach uses `oc` or `kubectl` commands directly. Note that while `kubectl` can create namespaces, the `oc new-project` command provides OpenShift-specific functionality.

#### Using oc (OpenShift CLI)

```bash
# Basic project creation
oc new-project my-ds-project

# With display name and description
oc new-project fraud-detection \
  --display-name="Fraud Detection ML" \
  --description="Machine learning models for fraud detection"

# Add the required label to make it visible in OpenShift AI
oc label project fraud-detection opendatahub.io/dashboard=true
```

#### Using kubectl

```bash
# Create a namespace (project)
kubectl create namespace my-ds-project

# Add required labels
kubectl label namespace my-ds-project opendatahub.io/dashboard=true

# Add annotations
kubectl annotate namespace my-ds-project \
  openshift.io/display-name="My Data Science Project" \
  openshift.io/description="Project for ML experiments"
```

## Listing and Viewing Projects

### List All Projects

```bash
# List all projects
kubectl get projects

# List with additional information
kubectl get projects -o wide

# List only data science projects
kubectl get projects -l opendatahub.io/dashboard=true

# Custom output showing key fields
kubectl get projects -o custom-columns=\
NAME:.metadata.name,\
DISPLAY:.metadata.annotations.openshift\\.io/display-name,\
STATUS:.status.phase
```

### View Specific Project

```bash
# Get project details
kubectl get project my-ds-project

# Get detailed description
kubectl describe project my-ds-project

# Get project in YAML format
kubectl get project my-ds-project -o yaml

# Get project in JSON format (useful for parsing)
kubectl get project my-ds-project -o json
```

### Filter Projects

```bash
# List projects by label
kubectl get projects -l team=data-science

# List projects by multiple labels
kubectl get projects -l opendatahub.io/dashboard=true,environment=production

# List projects with specific annotation (using jsonpath)
kubectl get projects -o jsonpath='{.items[?(@.metadata.annotations.openshift\.io/requester=="john.doe@example.com")].metadata.name}'
```

## Updating Projects

### Using kubectl apply (Declarative)

Modify your YAML file and reapply:
```bash
kubectl apply -f project-updated.yaml
```

### Using kubectl edit (Interactive)

```bash
# Opens project in your default editor
kubectl edit project my-ds-project
```

### Using kubectl patch

#### Update Annotations

```bash
# Add or update single annotation
kubectl annotate project my-ds-project \
  openshift.io/description="Updated ML project description" \
  --overwrite

# Add multiple annotations
kubectl annotate project my-ds-project \
  project.version="2.0" \
  project.owner="ml-team@example.com" \
  --overwrite

# Remove an annotation
kubectl annotate project my-ds-project project.version-
```

#### Update Labels

```bash
# Add or update labels
kubectl label project my-ds-project \
  environment=staging \
  compliance=hipaa \
  --overwrite

# Remove a label
kubectl label project my-ds-project compliance-
```

#### Using JSON Patch

```bash
# Update display name using JSON patch
kubectl patch project my-ds-project --type='json' \
  -p='[{"op": "replace", "path": "/metadata/annotations/openshift.io~1display-name", "value": "New Display Name"}]'

# Add multiple labels using merge patch
kubectl patch project my-ds-project --type='merge' \
  -p='{"metadata":{"labels":{"tier":"gpu-compute","cost-center":"ml-research"}}}'
```

## Deleting Projects

### Basic Deletion

```bash
# Delete a specific project
kubectl delete project my-ds-project

# Delete using YAML file
kubectl delete -f project.yaml

# Force deletion (use with caution)
kubectl delete project my-ds-project --force --grace-period=0
```

### Important Notes on Deletion

1. **Project deletion is irreversible** - All resources within the project will be deleted
2. **Terminating state** - Projects enter a "Terminating" state before complete removal
3. **Finalizers** - Some resources may have finalizers that prevent immediate deletion
4. **PVCs** - PersistentVolumeClaims might retain data depending on reclaim policy

### Check Deletion Status

```bash
# Monitor project deletion
kubectl get project my-ds-project -w

# Check for resources preventing deletion
kubectl api-resources --verbs=list --namespaced -o name \
  | xargs -n 1 kubectl get --show-kind --ignore-not-found -n my-ds-project
```

## Practical Examples

### Example 1: Create a Complete Data Science Project

```bash
# Create project YAML
cat <<EOF > datascience-project.yaml
apiVersion: project.openshift.io/v1
kind: Project
metadata:
  name: customer-churn-analysis
  labels:
    opendatahub.io/dashboard: 'true'
    kubernetes.io/metadata.name: customer-churn-analysis
    project-type: ml-classification
    team: customer-analytics
    cost-center: marketing
  annotations:
    openshift.io/display-name: Customer Churn Analysis
    openshift.io/description: 'ML models to predict customer churn using historical data'
    openshift.io/requester: sarah.chen@example.com
    project.start-date: '2024-01-15'
    project.ml-framework: 'pytorch,scikit-learn'
spec: {}
EOF

# Apply the project
kubectl apply -f datascience-project.yaml

# Verify creation
kubectl get project customer-churn-analysis
```

### Example 2: Migrate Existing Project to Data Science

```bash
# Add data science label to existing project
kubectl label project existing-project opendatahub.io/dashboard=true

# Update annotations for better organization
kubectl annotate project existing-project \
  openshift.io/display-name="Migrated ML Project" \
  openshift.io/description="Legacy project now enabled for OpenShift AI" \
  migration.date="$(date +%Y-%m-%d)" \
  --overwrite
```

### Example 3: Bulk Operations on Projects

```bash
# Add cost tracking label to all data science projects
kubectl get projects -l opendatahub.io/dashboard=true -o name | \
  xargs -I {} kubectl label {} cost-tracking=enabled --overwrite

# Export all data science projects
kubectl get projects -l opendatahub.io/dashboard=true -o yaml > all-ds-projects.yaml

# List projects with their descriptions
kubectl get projects -l opendatahub.io/dashboard=true \
  -o custom-columns=NAME:.metadata.name,DESCRIPTION:.metadata.annotations.openshift\\.io/description
```

## Verification and Troubleshooting

### Verify Project in OpenShift AI Dashboard

1. Check the label is present:
```bash
kubectl get project my-ds-project -o jsonpath='{.metadata.labels.opendatahub\.io/dashboard}'
```

2. Verify project appears in dashboard (via API):
```bash
# List all projects visible to OpenShift AI
kubectl get projects -l opendatahub.io/dashboard=true
```

### Common Issues and Solutions

#### Project Not Visible in Dashboard

```bash
# Check if label exists
kubectl get project my-ds-project --show-labels

# Add missing label
kubectl label project my-ds-project opendatahub.io/dashboard=true --overwrite
```

#### Permission Denied

```bash
# Check your permissions
kubectl auth can-i create projects

# Check specific project access
kubectl auth can-i get project my-ds-project
```

#### Project Stuck in Terminating

```bash
# Check what's preventing deletion
kubectl get all -n my-ds-project

# Check for finalizers
kubectl get project my-ds-project -o jsonpath='{.metadata.finalizers}'

# Remove finalizers if needed (use with caution)
kubectl patch project my-ds-project -p '{"metadata":{"finalizers":[]}}' --type=merge
```

## Best Practices

### Naming Conventions

1. **Use lowercase letters, numbers, and hyphens only**
   - Good: `ml-fraud-detection`, `customer-churn-v2`
   - Bad: `ML_Fraud_Detection`, `Customer.Churn`

2. **Include purpose in the name**
   - Good: `image-classification-prod`, `nlp-sentiment-dev`
   - Bad: `project1`, `test`

3. **Avoid generic names**
   - Use specific, descriptive names that indicate the project's purpose

### Label and Annotation Strategy

1. **Required Labels**
   ```yaml
   labels:
     opendatahub.io/dashboard: 'true'  # Required for OpenShift AI
   ```

2. **Recommended Labels**
   ```yaml
   labels:
     team: data-science              # Team ownership
     environment: development        # dev/staging/production
     project-type: ml-training      # Project category
     cost-center: ml-research       # Cost tracking
   ```

3. **Useful Annotations**
   ```yaml
   annotations:
     openshift.io/display-name: "Human Readable Name"
     openshift.io/description: "Detailed project description"
     openshift.io/requester: "email@example.com"
     project.docs.url: "https://docs.example.com/project"
     project.git.url: "https://github.com/org/repo"
   ```

### Security Considerations

1. **Limit project creation** to authorized users
2. **Use ResourceQuotas** to prevent resource exhaustion
3. **Apply NetworkPolicies** for network isolation
4. **Regular cleanup** of unused projects
5. **Audit project access** periodically

### When to Use Declarative vs Imperative

**Use Declarative (YAML) when:**
- Creating projects in production
- Need version control
- Automating with CI/CD
- Creating multiple related resources
- Need reproducible deployments

**Use Imperative (Commands) when:**
- Quick testing or development
- One-time operations
- Interactive troubleshooting
- Simple label/annotation updates

## Field Reference

| Field Path | Type | Required | Description | Example |
|------------|------|----------|-------------|---------|
| `apiVersion` | string | Yes | API version for Project resource | `project.openshift.io/v1` |
| `kind` | string | Yes | Resource type | `Project` |
| `metadata.name` | string | Yes | Project name (lowercase, hyphens) | `my-ds-project` |
| `metadata.labels` | object | No* | Key-value pairs for organization | `team: data-science` |
| `metadata.labels."opendatahub.io/dashboard"` | string | Yes** | Enable OpenShift AI integration | `'true'` |
| `metadata.labels."kubernetes.io/metadata.name"` | string | Auto | Automatically set to match name | `my-ds-project` |
| `metadata.annotations` | object | No | Non-identifying metadata | See below |
| `metadata.annotations."openshift.io/display-name"` | string | No | Human-readable name | `My Data Science Project` |
| `metadata.annotations."openshift.io/description"` | string | No | Project description | `ML experiments for customer analysis` |
| `metadata.annotations."openshift.io/requester"` | string | No | Project creator/owner | `john.doe@example.com` |
| `spec` | object | Yes | Project specification (usually empty) | `{}` |
| `status` | object | Read-only | Project status (set by system) | N/A |

\* Labels are optional but `opendatahub.io/dashboard` is required for OpenShift AI integration  
\** Required only for data science projects to appear in OpenShift AI dashboard

### Common Custom Annotations

| Annotation | Description | Example |
|------------|-------------|---------|
| `project.version` | Project version tracking | `'1.2.0'` |
| `project.owner` | Project owner/team | `ml-ops-team` |
| `project.docs.url` | Documentation link | `https://wiki.example.com/project` |
| `project.git.url` | Source code repository | `https://github.com/org/repo` |
| `project.jira.key` | Issue tracking reference | `MLOPS-123` |
| `project.start-date` | Project start date | `'2024-01-15'` |
| `project.ml-framework` | ML frameworks used | `tensorflow,pytorch` |
| `project.compliance` | Compliance requirements | `hipaa,pci-dss` |

## Using with Kubernetes MCP Server

If you're using the [Kubernetes MCP server](https://github.com/manusa/kubernetes-mcp-server) for AI-assisted operations, you'll need to adapt some commands since MCP tools work differently than direct kubectl commands.

### MCP Tool Mapping

| kubectl Command | MCP Tool | Parameters |
|----------------|----------|------------|
| `kubectl apply -f project.yaml` | `resources_create_or_update` | Pass YAML content as `resource` |
| `kubectl get projects` | `projects_list` | No parameters needed |
| `kubectl get project <name>` | `resources_get` | `apiVersion`, `kind`, `name` |
| `kubectl get projects -l <label>` | `resources_list` | `apiVersion`, `kind`, `labelSelector` |
| `kubectl delete project <name>` | `resources_delete` | `apiVersion`, `kind`, `name` |

### Creating Projects with MCP

Use the `resources_create_or_update` tool with the YAML content:

```yaml
# Pass this YAML to the resources_create_or_update tool
apiVersion: project.openshift.io/v1
kind: Project
metadata:
  name: my-ds-project
  labels:
    opendatahub.io/dashboard: 'true'
    kubernetes.io/metadata.name: my-ds-project
  annotations:
    openshift.io/display-name: My Data Science Project
    openshift.io/description: 'Project for ML experiments'
spec: {}
```

### Listing Projects with MCP

```bash
# List all OpenShift projects
# Use: projects_list (no parameters)

# List projects with specific labels
# Use: resources_list with parameters:
apiVersion: project.openshift.io/v1
kind: Project
labelSelector: opendatahub.io/dashboard=true
```

### Getting a Specific Project

```bash
# Use: resources_get with parameters:
apiVersion: project.openshift.io/v1
kind: Project
name: my-ds-project
```

### Updating Projects with MCP

Since MCP doesn't support `kubectl patch` or `kubectl label` directly:

1. **Get the current project** using `resources_get`
2. **Modify the YAML** (add/update labels or annotations)
3. **Apply the updated YAML** using `resources_create_or_update`

Example workflow:
```yaml
# 1. Get current project state
# 2. Modify the returned YAML to add a label:
metadata:
  labels:
    opendatahub.io/dashboard: 'true'
    environment: production  # New label
# 3. Pass modified YAML to resources_create_or_update
```

### Deleting Projects with MCP

```bash
# Use: resources_delete with parameters:
apiVersion: project.openshift.io/v1
kind: Project
name: my-ds-project
```

### MCP Limitations

The following operations from our documentation are not directly supported by MCP:

1. **Interactive editing** (`kubectl edit`) - Use get, modify, and update workflow instead
2. **Direct label/annotation commands** (`kubectl label`, `kubectl annotate`) - Update full resource
3. **JSONPath queries** - MCP returns full resources; filtering happens client-side
4. **Watch operations** (`-w` flag) - Not supported
5. **Custom output columns** - MCP returns standard formats
6. **Imperative namespace creation** - Use declarative YAML approach

### Best Practices for MCP

1. **Use declarative YAML** - This aligns perfectly with MCP's design
2. **Batch operations** - Get all resources and process them programmatically
3. **Full resource updates** - Always work with complete resource definitions
4. **Leverage projects_list** - Use the dedicated tool for listing OpenShift projects

## Related Resources

- [OpenShift Projects Documentation](https://docs.openshift.com/container-platform/latest/applications/projects/working-with-projects.html)
- [Kubernetes Namespaces](https://kubernetes.io/docs/concepts/overview/working-with-objects/namespaces/)
- [Red Hat OpenShift AI Documentation](https://docs.redhat.com/en/documentation/red_hat_openshift_ai_self-managed/)
- [OpenShift AI Dashboard Guide](https://docs.redhat.com/en/documentation/red_hat_openshift_ai_self-managed/2-latest/html/working_on_data_science_projects/)
- [Kubernetes MCP Server](https://github.com/manusa/kubernetes-mcp-server)

# Data Connections

Data Connections in Red Hat OpenShift AI provide secure access to external data sources and model registries. These connections enable workbenches, model serving, and pipelines to access S3-compatible object storage, model files on persistent volumes, and container registries without embedding credentials directly in your code.

## Overview

A data connection is a Kubernetes Secret with the label `opendatahub.io/dashboard: 'true'` and specific annotations that define the connection type. OpenShift AI recognizes these secrets and makes them available through the dashboard for use in:

- Jupyter workbenches (as environment variables)
- Model serving deployments
- Data science pipelines
- Direct API access for custom applications

### Connection Types

OpenShift AI supports two primary connection types:

1. **S3 Connections** (`opendatahub.io/connection-type: s3`)
   - For S3-compatible object storage (AWS S3, MinIO, OpenShift Data Foundation)
   - Used for datasets, model artifacts, pipeline storage, and model serving
   - Provides AWS-style credentials

2. **URI Connections** (`opendatahub.io/connection-type-ref: uri-v1`)
   - For storing model URIs that can be copied to InferenceService configurations
   - Supports PVC paths (`pvc://`), OCI registries (`oci://`), HTTP/HTTPS URLs
   - Provides a convenient way to manage and organize model locations
   - URI values are manually copied to `storageUri` in InferenceServices

### Required Labels and Annotations

All data connections must include:
- Label: `opendatahub.io/dashboard: 'true'` - Makes the connection visible in the dashboard
- Annotation: `openshift.io/display-name` - Human-readable name shown in the UI
- Connection type annotation - Defines how the connection is interpreted

## Creating Data Connections

### Method 1: Declarative (Using YAML)

The declarative approach is recommended for version control, automation, and reproducible deployments.

#### Basic S3 Connection

```yaml
# data-connection-s3-basic.yaml
apiVersion: v1
kind: Secret
metadata:
  name: my-s3-storage
  namespace: my-project
  labels:
    # Required: Makes connection available in OpenShift AI
    opendatahub.io/dashboard: 'true'
    # Optional: Indicates this is managed by the dashboard
    opendatahub.io/managed: 'true'
  annotations:
    # Required: Specifies this is an S3 connection
    opendatahub.io/connection-type: s3
    # Required: Display name in the dashboard
    openshift.io/display-name: My S3 Storage
type: Opaque
stringData:
  # Required: S3 access key ID
  AWS_ACCESS_KEY_ID: my-access-key
  # Required: S3 secret access key  
  AWS_SECRET_ACCESS_KEY: my-secret-key
  # Required: S3 endpoint URL
  AWS_S3_ENDPOINT: https://s3.amazonaws.com
  # Required: S3 bucket name
  AWS_S3_BUCKET: my-bucket
  # Optional: AWS region (defaults to us-east-1)
  AWS_DEFAULT_REGION: us-west-2
```

Apply the connection:
```bash
kubectl apply -f data-connection-s3-basic.yaml
```

#### Standard S3 Connection with Custom Endpoint

```yaml
# data-connection-s3-minio.yaml
apiVersion: v1
kind: Secret
metadata:
  name: minio-storage
  namespace: my-project
  labels:
    opendatahub.io/dashboard: 'true'
    opendatahub.io/managed: 'true'
  annotations:
    opendatahub.io/connection-type: s3
    openshift.io/display-name: MinIO Storage
    # Optional: Additional description
    openshift.io/description: 'Local MinIO instance for development'
type: Opaque
stringData:
  AWS_ACCESS_KEY_ID: minio
  AWS_SECRET_ACCESS_KEY: minio123
  # Custom endpoint for MinIO
  AWS_S3_ENDPOINT: https://minio-service.minio-namespace.svc.cluster.local:9000
  AWS_S3_BUCKET: ml-datasets
  AWS_DEFAULT_REGION: us-east-1
```

#### Basic URI Connection for PVC

```yaml
# data-connection-uri-pvc.yaml
apiVersion: v1
kind: Secret
metadata:
  name: model-pvc-connection
  namespace: my-project
  labels:
    opendatahub.io/dashboard: 'true'
  annotations:
    # Required: Specifies URI connection type
    opendatahub.io/connection-type-ref: uri-v1
    openshift.io/display-name: Model PVC Storage
    openshift.io/description: 'Points to models stored on PVC'
type: Opaque
stringData:
  # PVC URI format: pvc://<mount-path>/<model-path>
  URI: pvc://models/llama-2-7b-chat
```

#### URI Connection for OCI Registry

```yaml
# data-connection-uri-oci.yaml
apiVersion: v1
kind: Secret
metadata:
  name: model-registry-connection
  namespace: my-project
  labels:
    opendatahub.io/dashboard: 'true'
  annotations:
    opendatahub.io/connection-type-ref: uri-v1
    openshift.io/display-name: Red Hat Model Registry
type: Opaque
stringData:
  # OCI URI format for container registries
  URI: oci://registry.redhat.io/rhelai1/modelcar-granite-3-1-8b-instruct:1.5
```

### Method 2: Imperative (Using Commands)

#### Create S3 Connection

```bash
# Create secret with S3 credentials
kubectl create secret generic my-s3-connection \
  --from-literal=AWS_ACCESS_KEY_ID=my-key \
  --from-literal=AWS_SECRET_ACCESS_KEY=my-secret \
  --from-literal=AWS_S3_ENDPOINT=https://s3.amazonaws.com \
  --from-literal=AWS_S3_BUCKET=my-bucket \
  --from-literal=AWS_DEFAULT_REGION=us-east-1 \
  -n my-project

# Add required labels and annotations
kubectl label secret my-s3-connection \
  opendatahub.io/dashboard=true \
  opendatahub.io/managed=true \
  -n my-project

kubectl annotate secret my-s3-connection \
  opendatahub.io/connection-type=s3 \
  openshift.io/display-name="My S3 Connection" \
  -n my-project
```

#### Create URI Connection

```bash
# Create secret with URI
kubectl create secret generic model-uri-connection \
  --from-literal=URI=pvc://models/my-model \
  -n my-project

# Add required labels and annotations
kubectl label secret model-uri-connection \
  opendatahub.io/dashboard=true \
  -n my-project

kubectl annotate secret model-uri-connection \
  opendatahub.io/connection-type-ref=uri-v1 \
  openshift.io/display-name="Model URI Connection" \
  -n my-project
```

## Listing and Viewing Data Connections

### List All Data Connections

```bash
# List all data connections in current namespace
kubectl get secrets -l opendatahub.io/dashboard=true

# List with more details
kubectl get secrets -l opendatahub.io/dashboard=true \
  -o custom-columns=NAME:.metadata.name,TYPE:.metadata.annotations.opendatahub\.io/connection-type,DISPLAY:.metadata.annotations.openshift\.io/display-name

# List across all namespaces
kubectl get secrets --all-namespaces -l opendatahub.io/dashboard=true
```

### View Specific Connection

```bash
# View connection details (without showing secret data)
kubectl describe secret my-s3-connection

# View connection with decoded data (be careful with credentials)
kubectl get secret my-s3-connection -o jsonpath='{.data.AWS_ACCESS_KEY_ID}' | base64 -d

# View as YAML
kubectl get secret my-s3-connection -o yaml
```

### Filter Connections by Type

```bash
# List only S3 connections
kubectl get secrets -l opendatahub.io/dashboard=true \
  -o json | jq -r '.items[] | select(.metadata.annotations."opendatahub.io/connection-type" == "s3") | .metadata.name'

# List only URI connections
kubectl get secrets -l opendatahub.io/dashboard=true \
  -o json | jq -r '.items[] | select(.metadata.annotations."opendatahub.io/connection-type-ref" == "uri-v1") | .metadata.name'
```

## Updating Data Connections

### Using kubectl apply (Declarative)

Update your YAML file and reapply:

```bash
kubectl apply -f data-connection-s3-basic.yaml
```

### Using kubectl edit (Interactive)

```bash
# Edit connection interactively
kubectl edit secret my-s3-connection
```

### Using kubectl patch

#### Update S3 Credentials

```bash
# Update S3 credentials (base64 encode the values)
kubectl patch secret my-s3-connection --type='json' -p='[
  {"op": "replace", "path": "/data/AWS_ACCESS_KEY_ID", "value": "'$(echo -n "new-key" | base64)'"},
  {"op": "replace", "path": "/data/AWS_SECRET_ACCESS_KEY", "value": "'$(echo -n "new-secret" | base64)'"}
]'
```

#### Update Display Name

```bash
# Update display name annotation
kubectl annotate secret my-s3-connection \
  openshift.io/display-name="Production S3 Storage" \
  --overwrite
```

#### Add or Update S3 Endpoint

```bash
# Add new field or update existing
kubectl patch secret my-s3-connection --type='json' -p='[
  {"op": "add", "path": "/data/AWS_S3_ENDPOINT", "value": "'$(echo -n "https://new-endpoint.com" | base64)'"}
]'
```

## Deleting Data Connections

### Basic Deletion

```bash
# Delete a specific connection
kubectl delete secret my-s3-connection

# Delete multiple connections
kubectl delete secret my-s3-connection another-connection

# Delete by label selector
kubectl delete secrets -l opendatahub.io/dashboard=true,environment=dev
```

### Important Notes on Deletion

**WARNING**: Before deleting a data connection:
1. Ensure no workbenches are using the connection
2. Check that no model deployments reference the connection
3. Verify no pipelines depend on the connection
4. Consider backing up the connection configuration

### Check Connection Usage

```bash
# Check if any pods are using the S3 connection as environment variables
kubectl get pods -o json | jq -r '.items[] | select(.spec.containers[].envFrom[]?.secretRef.name == "my-s3-connection") | .metadata.name'

# Check if any InferenceServices reference S3 connections
kubectl get inferenceservices -o json | jq -r '.items[] | select(.spec.predictor.model.storage.key == "my-s3-connection") | .metadata.name'

# Note: URI connections are not directly referenced by InferenceServices
# Their values are manually copied to storageUri fields
```

## Practical Examples

### Example 1: Create S3 Connection for Dataset Storage

```yaml
# dataset-storage-connection.yaml
apiVersion: v1
kind: Secret
metadata:
  name: dataset-storage
  namespace: fraud-detection-project
  labels:
    opendatahub.io/dashboard: 'true'
    opendatahub.io/managed: 'true'
    purpose: dataset-storage
  annotations:
    opendatahub.io/connection-type: s3
    openshift.io/display-name: Fraud Detection Datasets
    openshift.io/description: 'S3 bucket containing fraud detection training data'
type: Opaque
stringData:
  AWS_ACCESS_KEY_ID: fraud-detection-key
  AWS_SECRET_ACCESS_KEY: fraud-detection-secret
  AWS_S3_ENDPOINT: https://s3.us-west-2.amazonaws.com
  AWS_S3_BUCKET: fraud-detection-datasets
  AWS_DEFAULT_REGION: us-west-2

# Model Serving

This section covers model serving capabilities in Red Hat OpenShift AI, including deployment strategies, configuration options, and best practices for serving machine learning models at scale.

## Available Serving Options

- **[Large Language Models (LLMs)](llms/)** - Deploy and serve LLMs from various storage sources including ModelCars, S3, and PVCs

# Useful Extras

This section contains supplementary guides and how-to documents that complement the main documentation. These guides provide detailed instructions for specific tasks that support multiple use cases across OpenShift AI.

## Available Guides

- **[Setting Up PVCs for Model Storage](model-pvc-setup/)** - Learn how to create and configure Persistent Volume Claims for storing and serving large language models
